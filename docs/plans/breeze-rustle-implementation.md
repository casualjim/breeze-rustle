# breeze-rustle Implementation Plan

## Overview

`breeze-rustle` is a Rust library that provides high-performance semantic code chunking using tree-sitter and nvim-treesitter queries. It will be published as a Python wheel so users don't need Rust toolchain.

## Project Goals

1. **Fast semantic chunking** - Parse code and split into semantic units with metadata
2. **nvim-treesitter queries** - Use community-maintained queries for 100+ languages
3. **Rich metadata** - Extract node types, scopes, definitions from code
4. **Python bindings** - Clean API for breeze to consume
5. **Zero copy where possible** - Efficient data transfer between Rust and Python

## Architecture

```
breeze-rustle/
├── Cargo.toml          # Rust dependencies
├── pyproject.toml      # Python packaging with maturin
├── build.rs            # Build script to fetch queries
├── src/
│   ├── lib.rs          # Main library interface
│   ├── queries.rs      # Generated by fetch-queries script
│   ├── chunker.rs      # Chunking logic
│   ├── parser.rs       # Tree-sitter parser management
│   └── python.rs       # PyO3 Python bindings
├── python/
│   └── breeze_rustle/
│       ├── __init__.py
│       └── __init__.pyi  # Type stubs
└── tests/
    ├── test_chunking.rs
    └── test_python.py
```

## Core API Design

### Rust API

```rust
// src/lib.rs
use pyo3::prelude::*;
use smol::future::Future;
use std::sync::Arc;

#[pyclass]
#[derive(Debug, Clone)]
pub struct ChunkMetadata {
    #[pyo3(get)]
    pub node_type: String,        // "function", "class", "method"
    #[pyo3(get)]
    pub node_name: Option<String>, // "parse_document", "MyClass"
    #[pyo3(get)]
    pub language: String,
    #[pyo3(get)]
    pub parent_context: Option<String>, // "class MyClass" for methods
    #[pyo3(get)]
    pub scope_path: Vec<String>,   // ["module", "MyClass", "parse_document"]
    #[pyo3(get)]
    pub definitions: Vec<String>,  // Variable/function names defined
    #[pyo3(get)]
    pub references: Vec<String>,   // Variable/function names referenced
}

#[pyclass]
#[derive(Debug, Clone)]
pub struct SemanticChunk {
    #[pyo3(get)]
    pub text: String,
    #[pyo3(get)]
    pub start_byte: usize,
    #[pyo3(get)]
    pub end_byte: usize,
    #[pyo3(get)]
    pub start_line: usize,
    #[pyo3(get)]
    pub end_line: usize,
    #[pyo3(get)]
    pub metadata: ChunkMetadata,
}

#[pyclass]
pub struct SemanticChunker {
    // Internal parser cache
    executor: Arc<smol::Executor<'static>>,
}

#[pymethods]
impl SemanticChunker {
    #[new]
    pub fn new(max_chunk_size: usize) -> PyResult<Self>;
    
    /// Chunk a single file into semantic units
    #[pyo3(signature = (content, language, file_path=None))]
    fn chunk_file<'p>(
        &mut self,
        py: Python<'p>,
        content: String,
        language: String,
        file_path: Option<String>,
    ) -> PyResult<&'p PyAny> {
        let chunker = self.clone();
        pyo3_asyncio::smol::future_into_py(py, async move {
            chunker.chunk_file_async(&content, &language, file_path.as_deref()).await
        })
    }
    
    /// Chunk multiple files concurrently
    #[pyo3(signature = (files))]
    fn chunk_files<'p>(
        &mut self,
        py: Python<'p>,
        files: Vec<(String, String, String)>, // (content, language, path)
    ) -> PyResult<&'p PyAny> {
        let chunker = self.clone();
        pyo3_asyncio::smol::future_into_py(py, async move {
            chunker.chunk_files_async(files).await
        })
    }
    
    /// Get supported languages
    #[staticmethod]
    pub fn supported_languages() -> Vec<String>;
    
    /// Check if a language is supported
    #[staticmethod]
    pub fn is_language_supported(language: &str) -> bool;
}

// Async implementation with flume channels
impl SemanticChunker {
    async fn chunk_file_async(
        &self,
        content: &str,
        language: &str,
        file_path: Option<&str>,
    ) -> PyResult<Vec<SemanticChunk>> {
        log::debug!("Chunking file: {:?} (language: {})", file_path, language);
        
        // Spawn parsing task on executor
        let task = self.executor.spawn(async move {
            // Do CPU-intensive parsing work
            log::trace!("Parsing {} bytes of {} code", content.len(), language);
            // ... parsing logic ...
        });
        task.await
    }
    
    async fn chunk_files_async(
        &self,
        files: Vec<(String, String, String)>,
    ) -> PyResult<Vec<Vec<SemanticChunk>>> {
        use futures_lite::StreamExt;
        
        // Create channel for work distribution
        let (tx, rx) = flume::bounded::<(usize, String, String, String)>(files.len());
        
        // Send work items
        for (idx, (content, lang, path)) in files.into_iter().enumerate() {
            tx.send_async((idx, content, lang, path)).await?;
        }
        drop(tx);
        
        // Process concurrently with controlled parallelism
        let mut results = vec![Vec::new(); rx.len()];
        let stream = rx.into_stream();
        
        stream
            .for_each_concurrent(num_cpus::get(), |(idx, content, lang, path)| async move {
                match self.chunk_file_async(&content, &lang, Some(&path)).await {
                    Ok(chunks) => {
                        log::info!("Chunked {}: {} chunks", path, chunks.len());
                        results[idx] = chunks;
                    }
                    Err(e) => {
                        log::error!("Failed to chunk {}: {}", path, e);
                    }
                }
            })
            .await;
        
        Ok(results)
    }
}

// Module initialization with logging
#[pymodule]
fn breeze_rustle(py: Python<'_>, m: &PyModule) -> PyResult<()> {
    // Initialize logging bridge
    pyo3_log::init();
    
    m.add_class::<SemanticChunker>()?;
    m.add_class::<SemanticChunk>()?;
    m.add_class::<ChunkMetadata>()?;
    Ok(())
}
```

### Python API

```python
# python/breeze_rustle/__init__.pyi
from typing import List, Optional, Tuple, Awaitable

class ChunkMetadata:
    node_type: str
    node_name: Optional[str]
    language: str
    parent_context: Optional[str]
    scope_path: List[str]
    definitions: List[str]
    references: List[str]

class SemanticChunk:
    text: str
    start_byte: int
    end_byte: int
    start_line: int
    end_line: int
    metadata: ChunkMetadata

class SemanticChunker:
    def __init__(self, max_chunk_size: int = 16384) -> None: ...
    
    async def chunk_file(
        self,
        content: str,
        language: str,
        file_path: Optional[str] = None
    ) -> List[SemanticChunk]: ...
    
    async def chunk_files(
        self,
        files: List[Tuple[str, str, str]]  # [(content, language, path), ...]
    ) -> List[List[SemanticChunk]]: ...
    
    @staticmethod
    def supported_languages() -> List[str]: ...
    
    @staticmethod
    def is_language_supported(language: str) -> bool: ...

# Python usage example:
# import asyncio
# from breeze_rustle import SemanticChunker
#
# async def main():
#     chunker = SemanticChunker()
#     chunks = await chunker.chunk_file("def foo(): pass", "python")
#     print(f"Found {len(chunks)} chunks")
#
# asyncio.run(main())
```

## Acceptance Criteria

### 1. Query Integration

- [ ] `fetch-queries` script runs in build.rs at compile time
- [ ] All nvim-treesitter queries are embedded in the binary
- [ ] Queries include locals.scm, highlights.scm, textobjects.scm
- [ ] No runtime file I/O for queries

### 2. Language Support

- [ ] Support all languages that have both tree-sitter parser and nvim-treesitter queries
- [ ] Graceful fallback for languages without queries
- [ ] Runtime detection of supported languages

### 3. Chunking Algorithm

- [ ] Use `@local.scope` from locals.scm to identify semantic boundaries
- [ ] Respect max_chunk_size parameter (default 16384 tokens)
- [ ] Never split in the middle of a semantic unit if possible
- [ ] Handle files with no semantic units gracefully

### 4. Metadata Extraction

- [ ] Extract node_type from tree-sitter node type
- [ ] Extract node_name from identifier nodes
- [ ] Build scope_path by traversing parent scopes
- [ ] Extract definitions using `@local.definition` captures
- [ ] Extract references using `@local.reference` captures
- [ ] Include parent_context (e.g., class name for methods)

### 5. Performance

- [ ] Parse 1MB Python file in <100ms
- [ ] Cache parsers between calls
- [ ] Minimal memory allocations
- [ ] Zero-copy where possible between Rust and Python
- [ ] Concurrent processing of multiple files
- [ ] Efficient async executor usage

### 6. Error Handling

- [ ] Return empty Vec for unparseable content
- [ ] Never panic on malformed code
- [ ] Helpful error messages for unsupported languages

### 7. Python Integration

- [ ] Published as wheel on PyPI
- [ ] No Rust toolchain required for users
- [ ] Type stubs for IDE support
- [ ] Works on Linux, macOS, Windows

## Acceptance Test Suite

```python
# tests/test_acceptance.py
import pytest
from breeze_rustle import SemanticChunker, SemanticChunk

class TestBasicFunctionality:
    def test_supported_languages(self):
        """Should support at least 50 languages"""
        languages = SemanticChunker.supported_languages()
        assert len(languages) >= 50
        assert "python" in languages
        assert "rust" in languages
        assert "javascript" in languages
    
    def test_simple_python_file(self):
        """Should chunk a simple Python file correctly"""
        content = '''
def hello(name):
    message = f"Hello {name}"
    return message

class Greeter:
    def __init__(self, lang):
        self.lang = lang
    
    def greet(self, name):
        return hello(name)
'''
        chunker = SemanticChunker()
        chunks = chunker.chunk_file(content, "python")
        
        # Should identify 3 semantic units: hello function, Greeter class, greet method
        assert len(chunks) >= 3
        
        # Check first chunk (hello function)
        hello_chunk = next(c for c in chunks if c.metadata.node_name == "hello")
        assert hello_chunk.metadata.node_type == "function"
        assert hello_chunk.metadata.definitions == ["message"]
        assert "name" in hello_chunk.metadata.references
        
        # Check class chunk
        class_chunk = next(c for c in chunks if c.metadata.node_name == "Greeter")
        assert class_chunk.metadata.node_type == "class"
        
        # Check method chunk
        greet_chunk = next(c for c in chunks if c.metadata.node_name == "greet")
        assert greet_chunk.metadata.node_type == "method"
        assert greet_chunk.metadata.parent_context == "class Greeter"
        assert greet_chunk.metadata.scope_path[-2:] == ["Greeter", "greet"]

class TestChunkingStrategy:
    def test_respects_max_chunk_size(self):
        """Should respect maximum chunk size"""
        # Create a large function
        content = '''
def process_data(items):
''' + '\n'.join([f'    item_{i} = items[{i}]' for i in range(1000)]) + '''
    return sum(items)
'''
        chunker = SemanticChunker(max_chunk_size=1000)  # Small limit
        chunks = chunker.chunk_file(content, "python")
        
        # Should split the large function
        assert len(chunks) > 1
        for chunk in chunks:
            assert len(chunk.text) < 5000  # Rough char estimate
    
    def test_preserves_semantic_boundaries(self):
        """Should not split semantic units unnecessarily"""
        content = '''
def small_func1():
    return 1

def small_func2():
    return 2

def small_func3():
    return 3
'''
        chunker = SemanticChunker(max_chunk_size=10000)  # Large limit
        chunks = chunker.chunk_file(content, "python")
        
        # Each function should be its own chunk
        assert len(chunks) == 3
        assert all(c.metadata.node_type == "function" for c in chunks)

class TestMetadataExtraction:
    def test_scope_hierarchy(self):
        """Should correctly track scope hierarchy"""
        content = '''
class Outer:
    class Inner:
        def method(self):
            def nested():
                x = 1
                return x
            return nested()
'''
        chunker = SemanticChunker()
        chunks = chunker.chunk_file(content, "python")
        
        nested_chunk = next(c for c in chunks if c.metadata.node_name == "nested")
        assert nested_chunk.metadata.scope_path == ["module", "Outer", "Inner", "method", "nested"]
        assert nested_chunk.metadata.parent_context == "def method"
        assert "x" in nested_chunk.metadata.definitions
    
    def test_captures_definitions_and_references(self):
        """Should extract variable definitions and references"""
        content = '''
def calculate(a, b):
    result = a + b
    temp = result * 2
    return temp
'''
        chunker = SemanticChunker()
        chunks = chunker.chunk_file(content, "python")
        
        chunk = chunks[0]
        # Parameters are definitions
        assert "a" in chunk.metadata.definitions or "a" in chunk.metadata.references
        assert "b" in chunk.metadata.definitions or "b" in chunk.metadata.references
        # Local variables are definitions
        assert "result" in chunk.metadata.definitions
        assert "temp" in chunk.metadata.definitions

class TestErrorHandling:
    def test_malformed_code(self):
        """Should handle malformed code gracefully"""
        content = '''
def broken(
    print("unclosed function"
class Also broken
'''
        chunker = SemanticChunker()
        chunks = chunker.chunk_file(content, "python")
        # Should return something, not crash
        assert isinstance(chunks, list)
    
    def test_unsupported_language(self):
        """Should handle unsupported languages gracefully"""
        chunker = SemanticChunker()
        chunks = chunker.chunk_file("some content", "unknown-language")
        assert chunks == []  # Empty list for unsupported
    
    def test_empty_file(self):
        """Should handle empty files"""
        chunker = SemanticChunker()
        chunks = chunker.chunk_file("", "python")
        assert chunks == []

class TestPerformance:
    def test_large_file_performance(self):
        """Should parse large files quickly"""
        import time
        
        # Generate a large Python file
        lines = []
        for i in range(100):
            lines.append(f"def function_{i}(x, y):")
            lines.append(f"    result = x + y + {i}")
            lines.append(f"    return result")
            lines.append("")
        
        content = "\n".join(lines)
        
        chunker = SemanticChunker()
        start = time.time()
        chunks = chunker.chunk_file(content, "python")
        elapsed = time.time() - start
        
        assert len(chunks) == 100  # One per function
        assert elapsed < 0.1  # Should be very fast
    
    def test_parser_caching(self):
        """Should cache parsers between calls"""
        import time
        
        chunker = SemanticChunker()
        content = "def test(): pass"
        
        # First call - parser initialization
        start1 = time.time()
        chunker.chunk_file(content, "python")
        time1 = time.time() - start1
        
        # Second call - should use cached parser
        start2 = time.time()
        chunker.chunk_file(content, "python")
        time2 = time.time() - start2
        
        # Second call should be notably faster
        assert time2 < time1 * 0.5  # At least 2x faster

class TestConcurrentProcessing:
    def test_chunk_multiple_files(self):
        """Should process multiple files concurrently"""
        files = [
            ("def func1(): return 1", "python", "file1.py"),
            ("def func2(): return 2", "python", "file2.py"),
            ("function func3() { return 3; }", "javascript", "file3.js"),
            ("fn func4() -> i32 { 4 }", "rust", "file4.rs"),
        ]
        
        chunker = SemanticChunker()
        results = chunker.chunk_files(files)
        
        assert len(results) == 4
        assert all(len(chunks) > 0 for chunks in results)
        
        # Check each file was processed correctly
        assert results[0][0].metadata.node_name == "func1"
        assert results[1][0].metadata.node_name == "func2"
        assert results[2][0].metadata.language == "javascript"
        assert results[3][0].metadata.language == "rust"
    
    def test_concurrent_performance(self):
        """Concurrent processing should be faster than sequential"""
        import time
        
        # Generate many files
        files = []
        for i in range(50):
            content = f"def function_{i}():\n    return {i}"
            files.append((content, "python", f"file_{i}.py"))
        
        chunker = SemanticChunker()
        
        # Time concurrent processing
        start = time.time()
        results_concurrent = chunker.chunk_files(files)
        time_concurrent = time.time() - start
        
        # Time sequential processing
        start = time.time()
        results_sequential = []
        for content, lang, path in files:
            results_sequential.append(chunker.chunk_file(content, lang, path))
        time_sequential = time.time() - start
        
        # Concurrent should be notably faster
        assert time_concurrent < time_sequential * 0.7  # At least 30% faster
        assert len(results_concurrent) == len(results_sequential)
```

## Implementation Notes

### Build Process

1. `build.rs` runs `fetch-queries` script
2. Generates `src/queries.rs` with embedded query strings
3. Compile everything into a static binary
4. maturin builds Python wheel

### Key Dependencies

- `tree-sitter` - Core parsing library
- `tree-sitter-loader` - Language loading
- `pyo3` - Python bindings with `extension-module` feature
- `pyo3-asyncio` - Async/await support with `attributes` and `smol-runtime` features
- `pyo3-log` - Bridge Rust logs to Python logging system
- `maturin` - Build tool for Python wheels
- `smol` - Lightweight async runtime
- `futures-lite` - Async utilities
- `flume` - Fast multi-producer multi-consumer channels
- `dashmap` - Concurrent hashmap for parser cache
- `log` - Rust logging facade (integrates with Python via pyo3-log)
- `num_cpus` - CPU count detection for parallelism

### Performance Considerations

- Use `Arc<str>` for shared string data
- Cache compiled queries and parsers in `DashMap`
- Use `Cow<str>` to avoid unnecessary allocations
- `smol::spawn` for CPU-bound tasks on thread pool
- Release Python GIL during async operations
- Batch small files to reduce overhead
- Consider work-stealing for better load balancing

### Error Handling Strategy

- Never panic - use Result types
- Log warnings for unsupported features
- Return partial results when possible
- Clear error messages in Python exceptions

## Success Metrics

1. All acceptance tests pass
2. Benchmarks show <100ms for 1MB files
3. Wheel size <10MB
4. Works on Python 3.8+
5. No segfaults or memory leaks
