# Example configuration for Breeze

# Database configuration
db_dir = "./embeddings.db"


[indexer]
# Processing configuration
max_file_size = 5242880  # 5MB - files larger than this are skipped
max_parallel_files = 8   # Number of files to process concurrently

# embedding_provider = "local" | "openai" | "ollama" | "voyage" | "cohere"  # Choose your embedding provider
embedding_provider = "ollama"

# Optional: Override the automatic chunk size calculation
# By default, chunk size is calculated as 80% of the model's context length
# max_chunk_size = 512  # Maximum tokens per chunk

# Optional: Number of overlapping tokens between chunks (helps preserve context)
# chunk_overlap = 0.1

# Optional: Number of concurrent workers for embedding (for remote providers)
# embedding_workers = 4

# Optional: Number of threads for processing large files
# large_file_threads = 4


[embeddings.local]
model = "ibm-granite/granite-embedding-125m-english"  # Local model for embeddings
# Optional: Override if auto-detection fails
# context_length = 512
# embedding_dim = 384

[embeddings.openai]
model = "text-embedding-ada-002"
# Optional: API key can also be set via OPENAI_API_KEY env var
# api_key = "sk-..."
# Optional: Override defaults
# api_base = "https://api.openai.com/v1"  # For API-compatible providers
# context_length = 8191
# embedding_dim = 1536
# max_batch_size = 2048  # OpenAI allows large batches

# Rate limiting (optional - has sensible defaults)
# requests_per_minute = 3000
# tokens_per_minute = 1000000
# max_concurrent_requests = 50

[embeddings.ollama]
model = "nomic-embed-text:latest"  # Ollama model for embeddings
api_base = "http://localhost:11434/v1"  # Ollama API base URL
# Optional: Override if auto-detection fails
# context_length = 8192
# embedding_dim = 768
# max_batch_size = 512  # Local servers might need smaller batches

[embeddings.voyage]
api_key = "${VOYAGE_API_KEY}"  # Set your Voyage API key in the environment
tier = "tier1"  # "free" | "tier1" | "tier2" | "tier3" - affects rate limits
model = "voyage-code-3" # Best for code embeddings

# Optional overrides (Voyage has good auto-detection)
# context_length = 16000
# embedding_dim = 1024

# Voyage automatically adjusts batch sizes based on tier:
# - free: conservative batching
# - tier1-3: progressively larger batches
# max_batch_size = 128  # Override only if needed

[embeddings.cohere]
api_base = "https://api.cohere.com/v1"
model = "embed-english-v3.0"
# Optional: API key can also be set via COHERE_API_KEY env var
# api_key = "..."
# input_type = "search_document"  # "search_document" | "search_query"
# context_length = 512
# embedding_dim = 1024
