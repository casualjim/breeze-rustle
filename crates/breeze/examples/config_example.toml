# Example configuration for Breeze

# Database configuration
db_dir = "./embeddings.db"

[server]

[server.ports]
http = 8080  # HTTP port
https = 8443  # HTTPS port

[server.tls]
disabled = false  # Enable TLS

# use keypair files, this does not support automatic cert rotation atm
[server.tls.keypair]
tls_cert = "server.crt"  # Path to the TLS certificate
tls_key = "server.key"  # Path to the TLS private key

# Or use letsencrypt over http-01 challenge
# [server.tls.letsencrypt]
# domains = ["example.com", "www.example.com"]  # List of domains for TLS
# emails = ["someone@example.com"]  # List of contacts for TLS
# production = true  # Use production Let's Encrypt servers
# cert_dir = "./certs"  # Directory to store certificates


[indexer]
# Optional: Number of overlapping tokens between chunks (helps preserve context)
# chunk_overlap = 0.1

[indexer.limits]
file_size = "5M"  # 5M - files larger than this are skipped
# Optional: Override the automatic chunk size calculation
# max_chunk_size = 512  # Maximum tokens per chunk
# Provider guidance example (for certain remote providers): ~3000 chars with 1000-char overlap (~33%)
# You can approximate 3000 characters as ~750 tokens (â‰ˆ 4 chars/token)
# If using that provider, consider:
# [indexer]
# chunk_overlap = 0.33
# [indexer.limits]
# max_chunk_size = 750

[indexer.workers]
small_file = 8   # Number of files to process concurrently
large_file = 4  # Number of large files to process concurrently
batch_size = 256 # Number of chunks to collect in a batch betfore sending to the embeddings workers

[embeddings]
# The selected embedding provider, from the providers defined below
# embedding_provider = "local" | "openai" | "ollama" | "voyage" | "cohere" | "my-provider"
provider = "ollama"  # Default embedding provider, can be overridden in indexer
workers = 4  # Number of concurrent embedding requests

[embeddings.local]
model = "ibm-granite/granite-embedding-125m-english"  # Local model for embeddings
tokenizer = "characters"
# Optional: Override if auto-detection fails
# context_length = 512
# embedding_dim = 384


# [embeddings.openai]
# model = "text-embedding-ada-002"
# Optional: API key can also be set via OPENAI_API_KEY env var
# api_key = "sk-..."
# Optional: Override defaults
# api_base = "https://api.openai.com/v1"  # For API-compatible providers
# context_length = 8191
# embedding_dim = 1536
# max_batch_size = 2048  # OpenAI allows large batches
# max_tokens_per_request = 8191  # Optional: defaults to context_length if not specified

# Rate limiting for the provider
# requests_per_minute = 3000
# tokens_per_minute = 1000000
# max_concurrent_requests = 50

# OpenAI-compatible provider example: Codestral embeddings
[embeddings.mistral]
model = "codestral-embed"
api_base = "https://api.mistral.ai/v1"
# api_key = "${MISTRAL_API_KEY}"
# Model characteristics
context_length = 8192
embedding_dim = 1536
max_batch_size = 64
# Tokenizer for batching estimates (HuggingFace)
tokenizer = "hf:mistralai/Codestral-22B-v0.1"
# Rate limiting (adjust to account limits)
requests_per_minute = 60
tokens_per_minute = 1_000_000
max_concurrent_requests = 10
# Optional provider-specific output controls
# output_dtype = "float"   # or "int8", "uint8", "binary", "ubinary"
# output_dimension = 1024

[embeddings.ollama]
model = "nomic-embed-text:latest"  # Ollama model for embeddings
api_base = "http://localhost:11434/v1"  # Ollama API base URL
# Optional: Override if auto-detection fails
# context_length = 8192
# embedding_dim = 768
# max_batch_size = 512  # Local servers might need smaller batches
# max_tokens_per_request = 8192  # Optional: defaults to context_length if not specified

# Rate limiting for the provider
# requests_per_minute = 3000
# tokens_per_minute = 1000000
# max_concurrent_requests = 50

[embeddings.voyage]
# api_key = "${VOYAGE_API_KEY}"  # Set your Voyage API key in the environment
tier = "tier-1"  # "free" | "tier-1" | "tier-2" | "tier-3" - affects rate limits
model = "voyage-code-3" # Best for code embeddings

# Optional overrides (Voyage has good auto-detection)
# context_length = 16000
# embedding_dim = 1024

# Voyage automatically adjusts batch sizes based on tier:
# - free: conservative batching
# - tier1-3: progressively larger batches
# max_batch_size = 128  # Override only if needed

# [embeddings.cohere]
# api_base = "https://api.cohere.com/v1"
# model = "embed-english-v3.0"
# Optional: API key can also be set via COHERE_API_KEY env var
# api_key = "..."
# context_length = 512
# embedding_dim = 1024
# max_tokens_per_request = 512  # Optional: defaults to context_length if not specified

# Rate limiting for the provider
# requests_per_minute = 3000
# tokens_per_minute = 1000000
# max_concurrent_requests = 50

# [embeddings.my-provider]
# api_base = "https://api.myprovider.com/v1"
# model = "my-embedding-model"
# Optional: API key can also be set via MY_PROVIDER_API_KEY env var
# api_key = "..."
# context_length = 512
# embedding_dim = 1024
# max_tokens_per_request = 512  # Optional: defaults to context_length if not specified

# Rate limiting for the provider
# requests_per_minute = 3000
# tokens_per_minute = 1000000
# max_concurrent_requests = 50
